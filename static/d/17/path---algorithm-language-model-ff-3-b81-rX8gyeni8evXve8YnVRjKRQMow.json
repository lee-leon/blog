{"data":{"markdownRemark":{"html":"<p>Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction, Speech Recognition, Summarization, Question-Answering etc. The goal of language modelling is to estimate the probability distribution of various linguistic units, e.g., words, sentences etc.</p>\n<h2 id=\"language-model-categories\"><a href=\"#language-model-categories\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Language Model Categories</h2>\n<p>Language models (LM) can be classified into two categories: count-based and continuous-space LM. </p>\n<h3 id=\"count-based-lm\"><a href=\"#count-based-lm\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Count Based LM</h3>\n<p>The count based LM is based on the n-th order <strong>Markov Assumption</strong> and\nestimating n-gram probabilities via counting and subsequent smoothing. </p>\n<blockquote>\n<p><strong>Markov Assumption</strong>: One can make predictions for the future of the\nprocess based solely on its present state just as well as one could knowing\nthe process’s full history, hence independently from such history.[1]</p>\n</blockquote>\n<p>Probability of a sequence of words is,</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\inline&amp;space;\\LARGE&amp;space;p\n(W)&amp;space;=&amp;space;p(w_{1},&amp;space;w_{2},&amp;space;w_{3},&amp;space;...,&amp;space;w_{n})\" title=\"\\LARGE p(W) = p(w_{1}, w_{2}, w_{3}, ..., w_{n})\">\n</p>\n<p>Probability of n-th word given previous n-1 words is,</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\inline&amp;space;\\LARGE&amp;space;p(w_{n})&amp;space;=&amp;space;p(&amp;space;w_{n}&amp;space;|&amp;space;w_{1},&amp;space;w_{2},&amp;space;w_{3},&amp;space;...,&amp;space;w_{n&amp;space;-&amp;space;1})\" title=\"\\LARGE p(w_{n}) = p( w_{n} | w_{1}, w_{2}, w_{3}, ..., w_{n - 1})\">\n</p>\n<p>So, the probability of the sequence of word can be formalized as,</p>\n<p align=\"center>\n<img src=\"https://latex.codecogs.com/svg.latex?\\inline&space;\\LARGE&space;p(W)&space;=&space;p(w_{1},&space;w_{2},&space;w_{3},&space;...,&space;w_{n})&space;=&space;\\prod_{i&space;=&space;1}^{n}p(w_{i}|w_{1},&space;w_{2},...,w_{i&space;-&space;1})\" title=\"\\LARGE p(W) = p(w_{1}, w_{2}, w_{3}, ..., w_{n}) = \\prod_{i = 1}^{n}p(w_{i}|w_{1}, w_{2},...,w_{i - 1})\" />\n</p>\n<ul>\n<li><mark>Data sparsity problem</mark>. Zero probability to all of the\ncombination that\nwere\nnot encountered in the training corpus. </li>\n<li>\n<p>Solutions:</p>\n<ul>\n<li><mark>back-off</mark>:\n= XX\n=</li>\n<li><mark>Smoothing</mark>\n= yy</li>\n</ul>\n</li>\n<li>practical usability problem, the dimensionality is very large since there\nis a huge number of different combinations of values of the input variables.</li>\n<li>It replies on exact pattern, i.e. string or word sequence matching, and\ntherefore are in no way linguistically informed.</li>\n<li>Another problem is with Markov Assumption as dependency beyond the window\nis ignored.</li>\n</ul>\n<h3 id=\"continuous-space-lm\"><a href=\"#continuous-space-lm\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Continuous-Space LM</h3>\n<h4 id=\"feed-forward-neural-probabilistic-language-models-nplms\"><a href=\"#feed-forward-neural-probabilistic-language-models-nplms\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Feed-Forward Neural Probabilistic Language Models (NPLMs)</h4>\n<p>NPLMs is proposed to solve the problem of data sparsity. </p>\n<h2 id=\"implementation\"><a href=\"#implementation\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Implementation</h2>\n<h3 id=\"count-based-lm-1\"><a href=\"#count-based-lm-1\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Count Based LM</h3>\n<h4 id=\"n-gram-model\"><a href=\"#n-gram-model\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>N-Gram Model</h4>\n<h3 id=\"continuous-space-lm-1\"><a href=\"#continuous-space-lm-1\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Continuous-Space LM</h3>\n<h4 id=\"\"><a href=\"#\" aria-hidden=\"true\" class=\"anchor\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a></h4>\n<p>[1] Jurafsky, Dan. Speech &#x26; language processing. Pearson Education India, 2000.</p>","timeToRead":2,"excerpt":"Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction…","frontmatter":{"title":"Language Model","date":"August 02, 2018","category":"Natural Language Processing","tags":["Statistic Model","Neural Network"],"cover":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAPoAAAD6AG1e1JrAAABBUlEQVQoz31TCRKDIBDzT1XpI7VeX9TWC9/QsNGVInVmh+FI2GwWkq17XmNpzFTnGBFr68a5zq+wJEpGWBnBtC7imJOMu/siHasME42hSN8lNvfl55VhR1Wc5LUxAE0ComCiscOYjxHIiGwbKG+fs1f5nWxAGf7xleMDEs3pmIKeIL7aE2ICFzhHUc6/7g+ZqZxJKFWugOzxmGBzaUxEtgrGMSd+h1R8UFSyHYKFb1QYk2gtfoIfMs7Q4b54oDHa8KFMuYTaXrodaD7IzoyMJm3qWe1KVQtxo21jZHqzSrXyHp1+OsRHokbE+wwOcNZ7qljySVHX3cfgN5LkhmTViZzTvgzJX1o3ABS9ie63AAAAAElFTkSuQmCC","aspectRatio":1.3752244165170557,"src":"/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-20a9c.png","srcSet":"/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-014f3.png 480w,\n/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-20a9c.png 766w","srcWebp":"/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-216d2.webp","srcSetWebp":"/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-32025.webp 480w,\n/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-216d2.webp 766w","sizes":"(max-width: 766px) 100vw, 766px"},"resize":{"src":"/static/language_model-ea62df3e9da4bf9aac7601e9a6e35381-e0155.png"}}}},"fields":{"slug":"/algorithm/language-model","sourceInstanceName":"algorithm"}}},"pageContext":{"slug":"/algorithm/language-model","left":{"frontmatter":{"tags":["ML","Supervised Learning","Linear Model"],"category":"Machine Learning","title":"Linear Regression","cover":{"childImageSharp":{"resize":{"src":"/static/linear_regression-7c3cd8a2b47d956946f965791ba0d013-5cbc0.png"}}}},"fields":{"slug":"/algorithm/linear-regression"}},"right":{"frontmatter":{"tags":["ML","Supervised Learning"],"category":"Machine Learning","title":"Logistic Regression","cover":{"childImageSharp":{"resize":{"src":"/static/logistic_regression-7896fc03edce001b914b60c78f26682f-5cbc0.png"}}}},"fields":{"slug":"/algorithm/logistic-regression"}}}}