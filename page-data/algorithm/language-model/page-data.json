{"componentChunkName":"component---src-templates-algorithm-jsx","path":"/algorithm/language-model","result":{"data":{"markdownRemark":{"html":"<p>Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction, Speech Recognition, Summarization, Question-Answering etc. The goal of language modelling is to estimate the probability distribution of various linguistic units, e.g., words, sentences etc.</p>\n<h2 id=\"language-model-categories\" style=\"position:relative;\"><a href=\"#language-model-categories\" aria-label=\"language model categories permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Language Model Categories</h2>\n<p>Language models (LM) can be classified into two categories: count-based and continuous-space LM. </p>\n<h3 id=\"count-based-lm\" style=\"position:relative;\"><a href=\"#count-based-lm\" aria-label=\"count based lm permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Count Based LM</h3>\n<p>The count based LM is based on the n-th order <strong>Markov Assumption</strong> and\nestimating n-gram probabilities via counting and subsequent smoothing. </p>\n<blockquote>\n<p><strong>Markov Assumption</strong>: One can make predictions for the future of the\nprocess based solely on its present state just as well as one could knowing\nthe process’s full history, hence independently from such history.[1]</p>\n</blockquote>\n<p>Probability of a sequence of words is,</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\inline&amp;space;\\LARGE&amp;space;p\n(W)&amp;space;=&amp;space;p(w_{1},&amp;space;w_{2},&amp;space;w_{3},&amp;space;...,&amp;space;w_{n})\" title=\"\\LARGE p(W) = p(w_{1}, w_{2}, w_{3}, ..., w_{n})\">\n</p>\n<p>Probability of n-th word given previous n-1 words is,</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\inline&amp;space;\\LARGE&amp;space;p(w_{n})&amp;space;=&amp;space;p(&amp;space;w_{n}&amp;space;|&amp;space;w_{1},&amp;space;w_{2},&amp;space;w_{3},&amp;space;...,&amp;space;w_{n&amp;space;-&amp;space;1})\" title=\"\\LARGE p(w_{n}) = p( w_{n} | w_{1}, w_{2}, w_{3}, ..., w_{n - 1})\">\n</p>\n<p>So, the probability of the sequence of word can be formalized as,</p>\n<p align=\"center\">\n<img src=\"https://latex.codecogs.com/svg.latex?\\inline&amp;space;\\LARGE&amp;space;p(W)&amp;space;=&amp;space;p(w_{1},&amp;space;w_{2},&amp;space;w_{3},&amp;space;...,&amp;space;w_{n})&amp;space;=&amp;space;\\prod_{i&amp;space;=&amp;space;1}^{n}p(w_{i}|w_{1},&amp;space;w_{2},...,w_{i&amp;space;-&amp;space;1})\" title=\"\\LARGE p(W) = p(w_{1}, w_{2}, w_{3}, ..., w_{n}) = \\prod_{i = 1}^{n}p(w_{i}|w_{1}, w_{2},...,w_{i - 1})\">\n</p>\n<ul>\n<li>\n<p><mark>Data sparsity Issue</mark>. Zero probability to all of the\ncombination that\nwere\nnot encountered in the training corpus. </p>\n<ul>\n<li>Solutions</li>\n<li>\n<p>back-off</p>\n<ul>\n<li>XX  </li>\n</ul>\n</li>\n<li>\n<p>Smoothing</p>\n<ul>\n<li>y</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><mark>Limited Practical Usability</mark>. The dimensionality is very\nlarge since there is a huge number of different combinations of values of the input variables.</li>\n<li><mark>Not Linguistically informed.</mark> It replies on exact\npattern</mark>. I.E. string or word sequence matching, and\ntherefore are in no way linguistically informed.</li>\n<li><mark>Markov Assumption Issue.</mark> Dependency beyond the window\nis ignored</li>\n</ul>\n<h3 id=\"continuous-space-lm\" style=\"position:relative;\"><a href=\"#continuous-space-lm\" aria-label=\"continuous space lm permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Continuous-Space LM</h3>\n<h4 id=\"feed-forward-neural-probabilistic-language-models-nplms\" style=\"position:relative;\"><a href=\"#feed-forward-neural-probabilistic-language-models-nplms\" aria-label=\"feed forward neural probabilistic language models nplms permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Feed-Forward Neural Probabilistic Language Models (NPLMs)</h4>\n<p>NPLMs is proposed to solve the problem of data sparsity. </p>\n<h2 id=\"implementation\" style=\"position:relative;\"><a href=\"#implementation\" aria-label=\"implementation permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Implementation</h2>\n<h3 id=\"count-based-lm-1\" style=\"position:relative;\"><a href=\"#count-based-lm-1\" aria-label=\"count based lm 1 permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Count Based LM</h3>\n<h4 id=\"n-gram-model\" style=\"position:relative;\"><a href=\"#n-gram-model\" aria-label=\"n gram model permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>N-Gram Model</h4>\n<h3 id=\"continuous-space-lm-1\" style=\"position:relative;\"><a href=\"#continuous-space-lm-1\" aria-label=\"continuous space lm 1 permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a>Continuous-Space LM</h3>\n<h4 id=\"\" style=\"position:relative;\"><a href=\"#\" aria-label=\" permalink\" class=\"anchor before\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 74.1 74.1\" height=\"18\" width=\"18\">\n  <path d=\"M41.5 55.3c.9 0 1.3 1.1.7 1.7L30.3 68.9c-6.9 6.9-18.2 7-25.1.1v-.1c-6.9-7-6.8-18.2.1-25.1l15.2-15.2c6.9-6.9 18.2-7 25.1-.1.6.6 1.2 1.3 1.7 2 .3.4.2.9-.1 1.3l-3.5 3.5c-1.2 1.2-2.9 1.6-4.2.5-5.2-4.8-9.1-3.8-12.3-.6L12 50.6c-3.2 3.2-3.2 8.4 0 11.6 3.2 3.2 8.4 3.1 11.6 0l8.2-8.2c.3-.3.7-.4 1.1-.2 2.7 1 5.6 1.6 8.6 1.5zM68.9 5.1c-7-6.9-18.2-6.8-25.1.1L31.9 17.1c-.6.6-.2 1.7.7 1.7 3-.1 5.9.5 8.7 1.6.4.1.8.1 1.1-.2l8.2-8.2c3.2-3.2 8.4-3.2 11.6 0 3.2 3.2 3.1 8.4 0 11.6L46.9 38.7c-3.2 3.2-8.4 3.2-11.6.1l-.4-.4c-1.2-1.4-3.3-.8-4.5.3l-3.5 3.5c-.3.3-.4.9-.1 1.3.5.7 1.1 1.4 1.7 2 7 6.9 18.2 6.8 25.1-.1l15.2-15.2c7-6.8 7-18 .1-25.1.1.1 0 .1 0 0z\"/>\n</svg>\n</a></h4>\n<p>[1] Jurafsky, Dan. Speech &#x26; language processing. Pearson Education India, 2000.</p>","timeToRead":2,"excerpt":"Language Modeling is an important idea behind many Natural Language Processing tasks such as Machine Translation, Spelling Correction…","frontmatter":{"title":"Language Model","date":"August 02, 2018","category":"Natural Language Processing","tags":["Statistic Model","Neural Network"],"cover":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAAEc/UiUAAAACXBIWXMAAAPoAAAD6AG1e1JrAAABHUlEQVQoz21SCRLCIAzkT/Z6ZLVav+hRKvgGN1lIoXUmpSTZXEvc9z74a4fT4Ytzn26fWy+/MOsv3gfaDDes127TzbnpqyYVHWAIFdH91EF/X9oK/xgbhECeY7NM6ZLLqSAORXgCES0aocgmEeeGatUMlKA6W8K01o6zrgAS3Nxb38kNfZlatvY6S4+sVUWTEiaoomGFKffclghnQbxYmuQG8DGebLwXRMfbWAyaDW8GXzlVSq4mmScKaNgP9ldIKrOW+UrZgoM+hFduSCDpgYWy5jPstoOT7jaDlfcdzYfKMVuBxpLI9OhCd2bJFxhtCSq+Y6qs1BWbSxfrH/tyedqWb8niXof3uotUyfZ/woAANzFbbZO80sMlO7L9A6X6ABSwd7xPAAAAAElFTkSuQmCC","aspectRatio":1.3753581661891117,"src":"/static/ea62df3e9da4bf9aac7601e9a6e35381/8f19a/language_model.png","srcSet":"/static/ea62df3e9da4bf9aac7601e9a6e35381/0d3b3/language_model.png 480w,\n/static/ea62df3e9da4bf9aac7601e9a6e35381/8f19a/language_model.png 766w","srcWebp":"/static/ea62df3e9da4bf9aac7601e9a6e35381/2b9a6/language_model.webp","srcSetWebp":"/static/ea62df3e9da4bf9aac7601e9a6e35381/83f4f/language_model.webp 480w,\n/static/ea62df3e9da4bf9aac7601e9a6e35381/2b9a6/language_model.webp 766w","sizes":"(max-width: 766px) 100vw, 766px"},"resize":{"src":"/static/ea62df3e9da4bf9aac7601e9a6e35381/73f08/language_model.png"}}}},"fields":{"slug":"/algorithm/language-model","sourceInstanceName":"algorithm"}}},"pageContext":{"slug":"/algorithm/language-model","left":{"frontmatter":{"tags":["ML","Supervised Learning"],"category":"Machine Learning","title":"Logistic Regression","cover":{"childImageSharp":{"resize":{"src":"/static/7896fc03edce001b914b60c78f26682f/2a4de/logistic_regression.png"}}}},"fields":{"slug":"/algorithm/logistic-regression"}},"right":{"frontmatter":{"tags":["ML","Supervised Learning","Linear Model"],"category":"Machine Learning","title":"Linear Regression","cover":{"childImageSharp":{"resize":{"src":"/static/7c3cd8a2b47d956946f965791ba0d013/2a4de/linear_regression.png"}}}},"fields":{"slug":"/algorithm/linear-regression"}}}}}